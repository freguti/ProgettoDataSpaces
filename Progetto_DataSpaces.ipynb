{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Progetto_DataSpaces.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM+aRPVejIEt3omHYhj2nJq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/freguti/ProgettoDataSpaces/blob/main/Progetto_DataSpaces.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjaEyBz9uvKw"
      },
      "source": [
        "#**Analysis of \"Predicting Pulsar Star\"**\n",
        "\n",
        "Simone Soncin - s263094\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGPeKiMJDTMs"
      },
      "source": [
        "##**Indice**\n",
        "\n",
        " \n",
        "\n",
        "1. [Introduzione](#Introduzione)\n",
        "2. [Analisi Dataset](#Analisi)\n",
        "3. [PCA](#PCA)\n",
        "4. [Bilanciamento Datadet](#bilanciamento)\n",
        "5. [Classificazione](#classificazione)\n",
        "  *   [Logistic Regression](#lr)\n",
        "  *   [Support Vector Machine](#svm)\n",
        "  *   [Naïve Bayes Classifier](#nbc)\n",
        "  *   [Random Forest Classifier](#rfc)\n",
        "  *   [K-Neares Neighbors Classifier](#knn)\n",
        "6. [Conclusioni](#conclusioni)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ryFeLG06Dcox",
        "outputId": "fb4f8205-db52-4767-f26f-ac65ef1f4e34"
      },
      "source": [
        "#Show/Hide code button\n",
        "from IPython.display import HTML\n",
        "HTML('''<script>\n",
        "code_show=true; \n",
        "function code_toggle() {\n",
        " if (code_show){\n",
        " $('div.input').hide();\n",
        " } else {\n",
        " $('div.input').show();\n",
        " }\n",
        " code_show = !code_show\n",
        "} \n",
        "$( document ).ready(code_toggle);\n",
        "</script>\n",
        "<form action=\"javascript:code_toggle()\">Il codice è stato nascosto per permettere una lettura più fluida. Per mostrarlo cliccare qui<br><input type=\"submit\" value=\"Abilita/Disabilita codice\"></form>''')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<script>\n",
              "code_show=true; \n",
              "function code_toggle() {\n",
              " if (code_show){\n",
              " $('div.input').hide();\n",
              " } else {\n",
              " $('div.input').show();\n",
              " }\n",
              " code_show = !code_show\n",
              "} \n",
              "$( document ).ready(code_toggle);\n",
              "</script>\n",
              "<form action=\"javascript:code_toggle()\">Il codice è stato nascosto per permettere una lettura più fluida. Per mostrarlo cliccare qui<br><input type=\"submit\" value=\"Abilita/Disabilita codice\"></form>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muG-UBR4Sp0g"
      },
      "source": [
        "<a name=\"introduzione\"></a>\n",
        "##**Introduzione**\n",
        "In questa tesina mi dedicherò ad analizzare un dataset, manipolare i dati per utilizzarli in maniera corretta e successivamente applicare dei modelli predittivi, identificando quale si comporta meglio nel nostro caso di studio.\n",
        "\n",
        "Le stelle pulsar sono delle stelle di neutroni che emettono un potente segnale radio dai poli misurabile dalla terra. Una stella pulsar ruota ad altissima velocità, rendendo questo segnale radio misurabile a tratti e fornendogli un pattern periodico che varia per ogni stella pulsar.\n",
        "\n",
        "Il dataset che utilizzerò è disponibile al link: https://www.kaggle.com/colearninglounge/predicting-pulsar-starintermediate?select=pulsar_data_train.csv \n",
        "\n",
        "Questo dataset contiene una serie di parametri continui discretizzati, i cui primi 4 sono derivati dal \"integrated pulse profile\", mentre gli ultimi 4 parametri sono ottenuti in maniera analoga dalla \"DM-SNR curve\".\n",
        "\n",
        "- Integrated profile: è ottenuto dal segnale periodico emesso dalla pulsar ed è univoco per ognuna di esse. Per questo motivo è utilizzato per identificarle a discapito del loro nome. \n",
        "- DM-SNR Curve: Dispersion Measure of the Signal to Noise Ratio in poche parole è la densità degli elettroni liberi lungo la linea ottica.\n",
        "\n",
        "I nostri parametri sono stati ottenuti da questi due dati, applicando rispettivamente la media, la deviazione standard, la curtosi (indica quanto sono spesse le code della distribuzione) e la simmetria statistica (asimmetria della distribuzione di probabilità rispetto alla sua media).\n",
        "\n",
        "Ecco la lista dettagliata dei parametri:\n",
        "- **Mean of the integrated profile**\n",
        "- **Standard deviation of the integrated profile**\n",
        "- **Excess kurtosis of the integrated profile**\n",
        "- **Skewness of the integrated profile**\n",
        "- **Mean of the DM-SNR curve**\n",
        "- **Standard deviation of the DM-SNR curve**\n",
        "- **Excess kurtosis of the DM-SNR curve**\n",
        "- **Skewness of the DM-SNR curve**\n",
        "\n",
        "Infine è presente un ultimo parametro discreto che indica se il segnale appartiene ad una pulsar (target_class = 1) o no (target_class = 0).\n",
        "\n",
        "###**Strumenti Utilizzati**\n",
        "- **Pandas:** libreria software per la manipolazione e l'analisi dei dati\n",
        "- **Numpy:** estensione di Python che aggiunge supporto per vettori e matrici multidimensionali di grandi dimensioni\n",
        "- **Sklearn:** libreria di Python specializzata nell'apprendimento automatico \n",
        "- **Plotly:** Per i grafici è stata utilizzata la libreria chart studio, che permette di creare dei grafici interattivi in maniera semplice ed intuitiva. Questo strumento ha il limite di upload dati fisso a 500Kb. Per la creazione dei grafici a istogramma e a boxplot, ho dovuto dividere il grafico desiderato in 4 grafici più piccoli a causa del raggiungimento di tale limite. Ho provveduto ad abbinare le feature in base al calcolo statistico usato per l'estrazione dalla grandezza fisica misurata.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUhtNYajBu7q"
      },
      "source": [
        "##Utility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU9NzVhxkR6R"
      },
      "source": [
        "#Capture removes the cell's output\n",
        "%%capture\n",
        "#Imports\n",
        "import pandas as pd\n",
        "import seaborn\n",
        "import numpy as np\n",
        "import functools\n",
        "import ipywidgets as widgets\n",
        "import sys\n",
        "from google.colab import drive\n",
        "!pip install -U imbalanced-learn\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install nbconvert\n",
        "!pip install -q gwpy\n",
        "import plotly\n",
        "!pip install chart_studio\n",
        "import chart_studio\n",
        "import chart_studio.plotly as py\n",
        "import plotly.graph_objs as go\n",
        "import plotly.figure_factory as ff\n",
        "from prettytable import PrettyTable\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline,make_pipeline\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split, KFold, GridSearchCV, learning_curve, cross_val_score\n",
        "from sklearn.metrics import roc_auc_score,confusion_matrix,roc_curve,auc, accuracy_score, precision_score, recall_score, f1_score\n",
        "from numpy import mean\n",
        "from scipy.cluster import hierarchy as hc\n",
        "from sklearn.decomposition import PCA\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "#alla fine sarà da eseguire alla fine del notebook\n",
        "#%%capture\n",
        "#It's used to convert this notebook in HTML\n",
        "#!jupyter nbconvert --to html /content/drive/MyDrive/DataSpaces/Progetto_DataSpaces.ipynb"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hO-Q8hB5Vnt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b2f1348-d6d1-4db5-e3c5-d8f7e564517d"
      },
      "source": [
        "#Parameters\n",
        "%%capture\n",
        "chart_studio.tools.set_credentials_file(username='freguti', api_key='4T2iyEX3eZP4aJ8PEJye')\n",
        "HISTOGRAM_COLORS = {\"pulsar\" : \"#388004\",\n",
        "                    \"not_pulsar\" : \"#8B0000\"}\n",
        "drive.mount('/content/drive')\n",
        "BOX_OPACITY = 0.3\n",
        "COLOR_PALETTE = seaborn.color_palette(\"Blues_d\").as_hex()\n",
        "COLORSCALE_HEATMAP = [         [0.0, '#011f4b'],\n",
        "                [0.1111111111111111, '#03396c'], \n",
        "                [0.2222222222222222, '#005b96'], \n",
        "                [0.3333333333333333, '#2171b5'], \n",
        "                [0.4444444444444444, '#6497b1'], \n",
        "                [0.5555555555555556, '#6baed6'], \n",
        "                [0.6666666666666666, '#B0E2FF'], \n",
        "                [0.7777777777777778, '#b3cde0'], \n",
        "                [0.8888888888888888, '#bdd7e7'], \n",
        "                               [1.0, '#BFEFFF']] \n",
        "PALETTE_HEATMAP = [[0.0, '#F5FFFA'], \n",
        "                         [0.2, '#ADD8E6'], \n",
        "                         [0.4, '#87CEEB'],\n",
        "                         [0.6, '#87CEFA'], \n",
        "                         [0.8, '#40E0D0'], \n",
        "                         [1.0, '#00CED1']]\n",
        "\n",
        "LR_PARAM = [{\n",
        "    'clf__solver': ['liblinear'],\n",
        "    'clf__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
        "    'clf__penalty': ['l2', 'l1']\n",
        "},\n",
        "{\n",
        "    'clf__solver': ['newton-cg', 'lbfgs'], \n",
        "    'clf__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'clf__penalty': ['l2']\n",
        "}]\n",
        "\n",
        "SVM_PARAM = [\n",
        "{\n",
        "    'clf__kernel': ['linear'],\n",
        "    'clf__C': [0.1, 1, 10, 100],\n",
        "}, \n",
        "{\n",
        "    'clf__kernel': ['rbf'],\n",
        "    'clf__C': [0.1, 1, 10, 100],\n",
        "    'clf__gamma': [0.1, 1, 10, 100],\n",
        "}]\n",
        "\n",
        "SVM_PARAM2 = [\n",
        " {\n",
        "    'clf__kernel': ['rbf'],\n",
        "    'clf__C': [0.1, 1, 10],\n",
        "    'clf__gamma': [0.1, 1, 10],\n",
        "}]\n",
        "\n",
        "RFC_PARAM = {\n",
        "    'clf__max_depth': [50, 75, 100],\n",
        "    'clf__max_features': [\"sqrt\", \"log2\"],\n",
        "    'clf__criterion': ['gini', 'entropy'],\n",
        "    'clf__n_estimators': [100, 300, 500]\n",
        "}\n",
        "\n",
        "KNN_PARAM = {\n",
        "    'clf__n_neighbors': [2, 3, 5, 10, 15],\n",
        "    'clf__weights': ['uniform', 'distance'],\n",
        "    'clf__p': [1, 2, 10]\n",
        "}\n",
        "\n",
        "NBC_PARAM = {}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fexperimentsandconfigs%20https%3a%2f%2fwww.googleapis.com%2fauth%2fphotos.native&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/1AX4XfWhgBLPBkmmEifPoNUa3GKLx2bya15oXP4jXlhbDoajclWl52xoBmN4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQSFps_4CVyD"
      },
      "source": [
        "%%capture\n",
        "#Graph utility\n",
        "\n",
        "def hexToRGBA(hex, alpha):\n",
        "    r = int(hex[1:3], 16)\n",
        "    g = int(hex[3:5], 16)\n",
        "    b = int(hex[5:], 16)\n",
        "\n",
        "    if alpha:\n",
        "        return \"rgba(\" + str(r) + \", \" + str(g) + \", \" + str(b) + \", \" + str(alpha) + \")\"\n",
        "    else:\n",
        "        return \"rgb(\" + str(r) + \", \" + str(g) + \", \" + str(b) + \")\"\n",
        "\n",
        "def histogram_bar(tipo, data, col, visible=True, norm = \"\"):\n",
        "    \n",
        "    color = HISTOGRAM_COLORS[tipo]\n",
        "    \n",
        "    data_16 = data.astype(np.float16 ,copy=True)\n",
        "    \n",
        "    return go.Histogram(\n",
        "        x = data_16[col],\n",
        "        name = tipo,\n",
        "        marker = dict(color = color),\n",
        "        visible = visible,\n",
        "        opacity = 1,\n",
        "        histnorm=norm\n",
        "    )\n",
        "\n",
        "#fare l'istogramma con chart studio https://plotly.com/python/getting-started-with-chart-studio/\n",
        "def feature_histogram(dataset, title = \"\", feature = \"\"):\n",
        "\n",
        "  trace0 = histogram_bar(\"not_pulsar\", dataset[dataset['target_class'] == 0], feature)\n",
        "  trace1 = histogram_bar(\"pulsar\", dataset[dataset['target_class'] == 1], feature)\n",
        "  \n",
        "  data = [trace0, trace1]\n",
        "  \n",
        "  layout = dict(\n",
        "      title=title,\n",
        "      autosize=True,\n",
        "      yaxis=dict(\n",
        "          title='value',\n",
        "          automargin=True,\n",
        "      ),\n",
        "      legend=dict(\n",
        "          x=0,\n",
        "          y=1,\n",
        "      ),\n",
        "      barmode='group',\n",
        "      bargap=0.15,\n",
        "      bargroupgap=0.1\n",
        "  )\n",
        "  fig = dict(data=data, layout=layout)\n",
        "  return py.iplot(fig, filename=title)\n",
        "\n",
        "\n",
        "def on_slider_change(sender,dataset):\n",
        "  if sender['name'] == 'value':\n",
        "    selected_option = sender['owner'].value\n",
        "    print(selected_option)\n",
        "    feature_histogram(dataset = dataset, title = \"Distribuzione features\", feature = selected_option)\n",
        "\n",
        "def create_slider_controlled_histogram(col,pulsar,not_pulsar,first_plot,name):\n",
        "  pulsar_plot = []\n",
        "  not_pulsar_plot = []\n",
        "  hist_features,pulsar_plot,not_pulsar_plot = prepare_set(dataset.columns,first_plot,pulsar,not_pulsar)\n",
        "  active_index = 0\n",
        "\n",
        "  hist_not = [(histogram_bar('not_pulsar', not_pulsar_plot, col, False,'percent') if i != active_index\n",
        "            else histogram_bar('not_pulsar', not_pulsar_plot, col, True,'percent'))\n",
        "              for i, col in enumerate(hist_features)\n",
        "              ]\n",
        "  hist_pulsar = [(histogram_bar('pulsar', pulsar_plot, col, False,'percent') if i != active_index\n",
        "            else histogram_bar('pulsar', pulsar_plot, col, True,'percent'))\n",
        "              for i, col in enumerate(hist_features)\n",
        "              ]\n",
        "\n",
        "  total_data = hist_not + hist_pulsar\n",
        "  number_of_features = len(hist_features)\n",
        "  steps = []\n",
        "\n",
        "  for i in range(number_of_features):\n",
        "      step = dict(\n",
        "          method = 'restyle',\n",
        "          args = ['visible', [False] * number_of_features],\n",
        "          label = hist_features[i],\n",
        "      )\n",
        "      step['args'][1][i] = True\n",
        "      steps.append(step)\n",
        "      \n",
        "  sliders = [dict(\n",
        "      active = active_index,\n",
        "      currentvalue = dict(\n",
        "          prefix = \"Feature: \", \n",
        "          xanchor= 'center',\n",
        "      ),\n",
        "      pad = {\"t\": 50},\n",
        "      steps = steps,\n",
        "      len=1,\n",
        "  )]\n",
        "\n",
        "  layout = dict(\n",
        "      sliders=sliders,\n",
        "      autosize=True,\n",
        "      yaxis=dict(\n",
        "          title='valori',\n",
        "          automargin=True,\n",
        "      ),\n",
        "      legend=dict(\n",
        "          x=0,\n",
        "          y=1,\n",
        "      ),\n",
        "  )\n",
        "\n",
        "  fig = dict(data=total_data, layout=layout)\n",
        "\n",
        "  return py.iplot(fig, filename=name)  \n",
        "\n",
        "def create_box(type, data, col, visible=False):\n",
        "   \n",
        "    c = HISTOGRAM_COLORS[type]\n",
        "    data_16 = data.astype(np.float16 ,copy=True)\n",
        "    return go.Box(\n",
        "        y = data_16[col],\n",
        "        name = type,\n",
        "        marker = dict(color = c),\n",
        "        visible = visible,\n",
        "        opacity = BOX_OPACITY,\n",
        "    )\n",
        "\n",
        "def create_slider_controlled_Boxplot(col,pulsar,not_pulsar,first_plot,name):\n",
        "  pulsar_plot = []\n",
        "  not_pulsar_plot = []\n",
        "  box_features,pulsar_plot,not_pulsar_plot = prepare_set(dataset.columns,first_plot,pulsar,not_pulsar)\n",
        "  active_index = 0\n",
        "\n",
        "  box_not_pulsar = [(create_box('not_pulsar', not_pulsar_plot, col, False) if i != active_index\n",
        "            else create_box('not_pulsar', not_pulsar_plot, col, True))\n",
        "              for i, col in enumerate(box_features)\n",
        "              ]\n",
        "  box_pulsar = [(create_box('pulsar', pulsar_plot, col, False) if i != active_index\n",
        "              else create_box('pulsar', pulsar_plot, col, True))\n",
        "              for i, col in enumerate(box_features)\n",
        "              ]\n",
        "\n",
        "  data = box_not_pulsar + box_pulsar\n",
        "  number_of_features = len(box_features)\n",
        "  steps = []\n",
        "\n",
        "  for i in range(number_of_features):\n",
        "      step = dict(\n",
        "          method = 'restyle',  \n",
        "          args = ['visible', [False] * number_of_features],\n",
        "          label = box_features[i],\n",
        "      )\n",
        "      step['args'][1][i] = True # Toggle i'th trace to \"visible\"\n",
        "      steps.append(step)\n",
        "      \n",
        "  sliders = [dict(\n",
        "      active = active_index,\n",
        "      currentvalue = dict(\n",
        "          prefix = \"Feature: \", \n",
        "          xanchor= 'center',\n",
        "      ),\n",
        "      pad = {\"t\": 50},\n",
        "      steps = steps,\n",
        "      len=1,\n",
        "  )]\n",
        "\n",
        "  layout = dict(\n",
        "      sliders=sliders,\n",
        "      autosize=True,\n",
        "      yaxis=dict(\n",
        "          title='valori',\n",
        "          automargin=True,\n",
        "      ),\n",
        "      legend=dict(\n",
        "          x=0,\n",
        "          y=1,\n",
        "      ),\n",
        "  )\n",
        "\n",
        "  fig = dict(data=data, layout=layout)\n",
        "  return py.iplot(fig, filename=name)\n",
        "\n",
        "def prepare_set(col,first_plot,pulsar,not_pulsar): \n",
        "  pulsar_plot = pulsar\n",
        "  not_pulsar_plot = not_pulsar\n",
        "  box_features = col.values.tolist()\n",
        "  if first_plot == \"mean\":\n",
        "    #to_drop = [box_features[1],box_features[2],box_features[3],box_features[5],box_features[6],box_features[7],box_features[8]]\n",
        "    #pulsar_plot = pulsar.drop(to_drop,axis =1)\n",
        "    #not_pulsar_plot = not_pulsar.drop(to_drop,axis =1)\n",
        "    hist_features = [box_features[0],box_features[4]]\n",
        "  if first_plot == \"deviation\":\n",
        "    #to_drop = [box_features[0],box_features[2],box_features[3],box_features[4],box_features[6],box_features[7],box_features[8]]\n",
        "    #pulsar_plot = pulsar.drop(to_drop,axis =1)\n",
        "    #not_pulsar_plot = not_pulsar.drop(to_drop,axis =1)\n",
        "    hist_features = [box_features[1],box_features[5]]\n",
        "  if first_plot == \"kurtosis\":\n",
        "    #to_drop = [box_features[0],box_features[1],box_features[3],box_features[4],box_features[5],box_features[7],box_features[8]]\n",
        "    #pulsar_plot = pulsar.drop(to_drop,axis =1)\n",
        "    #not_pulsar_plot = not_pulsar.drop(to_drop,axis =1)\n",
        "    hist_features = [box_features[2],box_features[6]]\n",
        "  if first_plot == \"skewness\":\n",
        "    #to_drop = [box_features[0],box_features[1],box_features[2],box_features[4],box_features[5],box_features[6],box_features[8]]\n",
        "    #pulsar_plot = pulsar.drop(to_drop,axis =1)\n",
        "    #not_pulsar_plot = not_pulsar.drop(to_drop,axis =1)\n",
        "    hist_features = [box_features[3],box_features[7]]\n",
        "  return hist_features,pulsar_plot,not_pulsar_plot\n",
        "\n",
        "def plot_variance(pca, title):\n",
        "  tot_var = np.sum(pca.explained_variance_)\n",
        "  ex_var = [(i / tot_var) * 100 for i in sorted(pca.explained_variance_, reverse=True)]\n",
        "  cum_ex_var = np.cumsum(ex_var)\n",
        "\n",
        "  cum_var_bar = go.Bar(\n",
        "      x=list(range(1, len(cum_ex_var) + 1)), \n",
        "      y=ex_var,\n",
        "      name=\"Varianza di ogni componente\",\n",
        "      marker=dict(\n",
        "          color=HISTOGRAM_COLORS[\"pulsar\"],\n",
        "      ),\n",
        "      opacity= 1\n",
        "      )\n",
        "\n",
        "  variance_line = go.Scatter(\n",
        "      x=list(range(1, len(cum_ex_var) + 1)),\n",
        "      y=cum_ex_var,\n",
        "      mode='lines+markers',\n",
        "      name=\"Varianza cumulativa\",\n",
        "      marker=dict(\n",
        "          color=HISTOGRAM_COLORS[\"not_pulsar\"],\n",
        "      ),\n",
        "      opacity= 1,\n",
        "      line=dict(\n",
        "          shape='hv',\n",
        "      ))\n",
        "  data = [cum_var_bar, variance_line]\n",
        "  layout = go.Layout(\n",
        "      autosize=True,\n",
        "      title=title,\n",
        "      yaxis=dict(\n",
        "          title='Varianza (%)',\n",
        "      ),\n",
        "      xaxis=dict(\n",
        "          title=\"Componenti principali\",\n",
        "          dtick=1,\n",
        "          rangemode='nonnegative'\n",
        "      ),\n",
        "      legend=dict(\n",
        "          x=0,\n",
        "          y=1,\n",
        "      ),\n",
        "  )\n",
        "  fig = go.Figure(data=data, layout=layout)\n",
        "  return py.iplot(fig, filename=title)\n",
        "\n",
        "def PCA_Reduction(X_norm, plot = False , n_components = 8):\n",
        "  pca = PCA(random_state=42) #random seed\n",
        "  ortogonal = pca.fit_transform(X_norm)\n",
        "\n",
        "  if plot:\n",
        "      p = plot_variance(pca, \"Varianza singola e cumulativa\")\n",
        "\n",
        "  pca.components_ = pca.components_[:n_components]\n",
        "  reduced_data = np.dot(ortogonal, pca.components_.T)\n",
        "  X_red = pd.DataFrame(reduced_data, columns=[\"PC#%d\" % (x + 1) for x in range(n_components)])\n",
        "  if plot:\n",
        "      return p, X_red\n",
        "  else:\n",
        "      return X_red\n",
        "\n",
        "def grid_search_cv(model, params, X_train, y_train, cv):\n",
        "  #over = SMOTE(sampling_strategy=0.4)\n",
        "  #under = RandomUnderSampler(sampling_strategy=0.6)\n",
        "  #steps = [('over', over), ('under', under), ('model', model)]\n",
        "  #pipeline = Pipeline(steps=steps)\n",
        "  pipeline = Pipeline([('sampling', SMOTE(sampling_strategy = 0.3)),('sampl',RandomUnderSampler(sampling_strategy=1)),('clf', model)])\n",
        "  grid_search = GridSearchCV(estimator=pipeline, \n",
        "                              param_grid=params, \n",
        "                              cv=cv, \n",
        "                              n_jobs=-1,       # Use all processors\n",
        "                              scoring='f1',    # Use f1 metric for evaluation\n",
        "                              return_train_score=True)\n",
        "  grid_search.fit(X_train, y_train)\n",
        "  return grid_search\n",
        "\n",
        "def print_best_scores(grid_search, n=5):\n",
        "  t = PrettyTable()\n",
        "\n",
        "  print(\"Migliori parametri su insieme di validazione:\")\n",
        "  indexes = np.argsort(grid_search.cv_results_['mean_test_score'])[::-1][:n]\n",
        "  means = grid_search.cv_results_['mean_test_score'][indexes]\n",
        "  stds = grid_search.cv_results_['std_test_score'][indexes]\n",
        "  params = np.array(grid_search.cv_results_['params'])[indexes]\n",
        "  \n",
        "  t.field_names = ['Score'] + [f for f in params[0].keys()] \n",
        "  for mean, std, params in zip(means, stds, params):\n",
        "      if 'clf__kernel' in params.keys() and params['clf__kernel'] is 'linear':\n",
        "          params['clf__gamma'] = 'None'\n",
        "      row=[\"%0.3f (+/-%0.03f)\" % (mean, std * 2)] + [p for p in params.values()]\n",
        "      t.add_row(row)\n",
        "  print(t)\n",
        "\n",
        "def print_performances(classifiers, classifier_names, auc_scores, X_test, y_test):\n",
        "\n",
        "  accs = []\n",
        "  recalls = []\n",
        "  precision = []\n",
        "  results_table = pd.DataFrame(columns=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"auc\"])\n",
        "  for (i, clf), name, auc in zip(enumerate(classifiers), classifier_names, auc_scores):\n",
        "      y_pred = clf.predict(X_test)\n",
        "      row = []\n",
        "      row.append(accuracy_score(y_test, y_pred))\n",
        "      row.append(precision_score(y_test, y_pred))\n",
        "      row.append(recall_score(y_test, y_pred))\n",
        "      row.append(f1_score(y_test, y_pred))\n",
        "      row.append(auc)\n",
        "      row = [\"%.3f\" % r for r in row]\n",
        "      results_table.loc[name] = row\n",
        "  return results_table\n",
        "\n",
        "def plot_roc_curve(classifiers, legend, title, X_test, y_test):\n",
        "  \n",
        "  t1 = go.Scatter(\n",
        "      x=[0, 1], \n",
        "      y=[0, 1], \n",
        "      showlegend=False,\n",
        "      mode=\"lines\",\n",
        "      name=\"\",\n",
        "      line = dict(\n",
        "          color = COLOR_PALETTE[0],\n",
        "      ),\n",
        "  )\n",
        "  \n",
        "  data = [t1]\n",
        "  aucs = []\n",
        "  for clf, string, c in zip(classifiers, legend, COLOR_PALETTE[1:]):\n",
        "      y_test_roc = np.array([([0, 1] if y else [1, 0]) for y in y_test])\n",
        "      y_score = clf.predict_proba(X_test)\n",
        "      \n",
        "      # Compute ROC curve and ROC area for each class\n",
        "      fpr = dict()\n",
        "      tpr = dict()\n",
        "      roc_auc = dict()\n",
        "      for i in range(2):\n",
        "          fpr[i], tpr[i], _ = roc_curve(y_test_roc[:, i], y_score[:, i])\n",
        "          roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "      # Compute micro-average ROC curve and ROC area\n",
        "      fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_roc.ravel(), y_score.ravel())\n",
        "      roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "      aucs.append(roc_auc['micro'])\n",
        "\n",
        "      trace = go.Scatter(\n",
        "          x=fpr['micro'], \n",
        "          y=tpr['micro'], \n",
        "          showlegend=True,\n",
        "          mode=\"lines\",\n",
        "          name=string + \" (area = %0.2f)\" % roc_auc['micro'],\n",
        "          hoverlabel = dict(\n",
        "              namelength=30\n",
        "          ),\n",
        "          line = dict(\n",
        "              color = c,\n",
        "          ),\n",
        "      )\n",
        "      data.append(trace)\n",
        "\n",
        "  layout = go.Layout(\n",
        "      title=title,\n",
        "      autosize=False,\n",
        "      width=550,\n",
        "      height=550,\n",
        "      yaxis=dict(\n",
        "          title='True Positive Rate',\n",
        "      ),\n",
        "      xaxis=dict(\n",
        "          title=\"False Positive Rate\",\n",
        "      ),\n",
        "      legend=dict(\n",
        "          x=0.4,\n",
        "          y=0.06,\n",
        "      ),\n",
        "  )\n",
        "  fig = go.Figure(data=data, layout=layout)\n",
        "  return aucs, py.iplot(fig, filename=title)\n",
        "\n",
        "\n",
        "def plot_learning_curve(estimator, title, X, y, cv=None, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5), name = \"\"):\n",
        "\n",
        "  train_sizes, train_scores, test_scores = learning_curve(estimator, \n",
        "                                                          X, \n",
        "                                                          y, \n",
        "                                                          cv=cv, \n",
        "                                                          n_jobs=n_jobs, \n",
        "                                                          train_sizes=train_sizes, \n",
        "                                                          scoring=\"f1\", \n",
        "                                                          random_state=42,\n",
        "                                                          )\n",
        "  \n",
        "  train_scores_mean = np.mean(train_scores, axis=1)\n",
        "  train_scores_std = np.std(train_scores, axis=1)\n",
        "  test_scores_mean = np.mean(test_scores, axis=1)\n",
        "  test_scores_std = np.std(test_scores, axis=1)\n",
        "  \n",
        "  # Prints lower bound (mean - std) of train \n",
        "  trace1 = go.Scatter(\n",
        "      x=train_sizes, \n",
        "      y=train_scores_mean - train_scores_std, \n",
        "      showlegend=False,\n",
        "      mode=\"lines\",\n",
        "      name=\"\",\n",
        "      hoverlabel = dict(\n",
        "          namelength=20\n",
        "      ),\n",
        "      line = dict(\n",
        "          width = 0.1,\n",
        "          color = hexToRGBA(HISTOGRAM_COLORS[\"not_pulsar\"], 0.4),\n",
        "      ),\n",
        "  )\n",
        "  # Prints upper bound (mean + std) of train\n",
        "  trace2 = go.Scatter(\n",
        "      x=train_sizes, \n",
        "      y=train_scores_mean + train_scores_std, \n",
        "      showlegend=False,\n",
        "      fill=\"tonexty\",\n",
        "      mode=\"lines\",\n",
        "      name=\"\",\n",
        "      hoverlabel = dict(\n",
        "          namelength=20\n",
        "      ),\n",
        "      line = dict(\n",
        "          width = 0.1,\n",
        "          color = hexToRGBA(HISTOGRAM_COLORS[\"not_pulsar\"], 0.4),\n",
        "      ),\n",
        "  )\n",
        "  \n",
        "  # Prints mean train score line\n",
        "  trace3 = go.Scatter(\n",
        "      x=train_sizes, \n",
        "      y=train_scores_mean, \n",
        "      showlegend=True,\n",
        "      name=\"Punteggio training\",\n",
        "      line = dict(\n",
        "          color = HISTOGRAM_COLORS[\"not_pulsar\"],\n",
        "      ),\n",
        "  )\n",
        "  \n",
        "  # Prints lower bound (mean - std) of test \n",
        "  trace4 = go.Scatter(\n",
        "      x=train_sizes, \n",
        "      y=test_scores_mean - test_scores_std, \n",
        "      showlegend=False,\n",
        "      mode=\"lines\",\n",
        "      name=\"\",\n",
        "      hoverlabel = dict(\n",
        "          namelength=20\n",
        "      ),\n",
        "      line = dict(\n",
        "          width = 0.1,\n",
        "          color = hexToRGBA(HISTOGRAM_COLORS[\"pulsar\"], 0.4),\n",
        "      ),\n",
        "  )\n",
        "      # Prints upper bound (mean + std) of test\n",
        "  trace5 = go.Scatter(\n",
        "      x=train_sizes, \n",
        "      y=test_scores_mean + test_scores_std, \n",
        "      showlegend=False,\n",
        "      fill=\"tonexty\",\n",
        "      mode=\"lines\",\n",
        "      name=\"\",\n",
        "      hoverlabel = dict(\n",
        "          namelength=20\n",
        "      ),\n",
        "      line = dict(\n",
        "          width = 0.1,\n",
        "          color = hexToRGBA(HISTOGRAM_COLORS[\"pulsar\"], 0.4),\n",
        "      ),\n",
        "  )\n",
        "\n",
        "  # Prints mean test score line \n",
        "  trace6 = go.Scatter(\n",
        "      x=train_sizes, \n",
        "      y=test_scores_mean, \n",
        "      showlegend=True,\n",
        "      name=\"Punteggio test\",\n",
        "      line = dict(\n",
        "          color = HISTOGRAM_COLORS[\"pulsar\"],\n",
        "      ),\n",
        "  )\n",
        "  \n",
        "  data = [trace1, trace2, trace3, trace4, trace5, trace6]\n",
        "  layout = go.Layout(\n",
        "      title=title,\n",
        "      autosize=True,\n",
        "      yaxis=dict(\n",
        "          title='Punteggio',\n",
        "      ),\n",
        "      xaxis=dict(\n",
        "          title=\"#Dati training \",\n",
        "      ),\n",
        "      legend=dict(\n",
        "          x=0.8,\n",
        "          y=0,\n",
        "      ),\n",
        "  )\n",
        "  fig = go.Figure(data=data, layout=layout)\n",
        "  title_fin = title + \" [\" + name + \"]\"\n",
        "  return py.iplot(fig, filename= title_fin)\n",
        "\n",
        "\n",
        "def print_confusion_matrix(gs, X_test, y_test):\n",
        "\n",
        "  gs_score = gs.score(X_test, y_test)\n",
        "  y_pred = gs.predict(X_test)\n",
        "\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "  t = PrettyTable()\n",
        "  t.add_row([\"True not pulsar\", cm[0][0], cm[0][1]])\n",
        "  t.add_row([\"True pulsar\", cm[1][0], cm[1][1]])\n",
        "  t.field_names = [\" \", \"Predicted not pulsar\", \"Predicted pulsar\"]\n",
        "  print(t)\n",
        "\n",
        "  cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # normalize the confusion matrix\n",
        "  cm_df = pd.DataFrame(cm.round(3), index=[\"True not pulsar\", \"True pulsar\"], columns=[\"Predicted not pulsar\", \"Predicted pulsar\"])\n",
        "  cm_df\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jONDdFWm3tZZ"
      },
      "source": [
        "<a name=\"Analisi\"></a>\n",
        "##**Analisi Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfng4uxpmYN3"
      },
      "source": [
        "Come prima cosa è necessario eseguire un'analisi del dataset, eseguendo eventualmente delle operazioni per migliorarlo e renderlo più adatto al nostro tipo di analisi.\n",
        "\n",
        "La fase di analisi è la più importante, perchè permette di pulire il dataset migliorando l'analisi effettuata e ridurre la potenza di calcolo necessaria per eseguirla.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1leO2hjh7Ge",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48b689d8-8ea6-40ad-f919-c6e1280ee2cc"
      },
      "source": [
        "dataset = pd.read_csv('/content/drive/My Drive/DataSpaces/pulsar_stars.csv')\n",
        "for column in dataset:\n",
        " if dataset[column].dtype == 'float64':\n",
        "  dataset[column]=pd.to_numeric(dataset[column], downcast='float')\n",
        "  if dataset[column].min() <= np.finfo(np.float16).min or dataset[column].max() >= np.finfo(np.float16).max:\n",
        "    raise CustomError(\"Range Exception\")\n",
        " if dataset[column].dtype == 'int64':\n",
        "  dataset[column]=pd.to_numeric(dataset[column], downcast='integer')\n",
        "\n",
        "print(\"il dataset ha %d record e %d features\\n\\nNumero di valori null per ogni feature:\" % dataset.shape)\n",
        "print(dataset.isnull().sum())\n",
        "\n",
        "print(\"\\nDa quel che si può vedere, non ci sono valori nulli all'interno del nostro dataset. Nel caso in cui ne avessimo trovato uno avremo avuto due possibilità:\\n\")\n",
        "print(\"- Se i valori nulli non sono molti possiamo procedere eliminando i record che riportano il valore nullo;\\n- Se il numero di questi valori nulli è elevato si può procedere alla sostituzione con dei valori di default\\n\")\n",
        "\n",
        "print(\"ci sono 2 classi differenti.\\n0 -> non è una pulsar\\n1 -> è una pulsar\")\n",
        "X, y = make_classification(n_classes = 2, )\n",
        "target_class = dataset['target_class'].value_counts()\n",
        "print(\"numero di record per ogni classe:\\n\" + target_class.to_string() + \"\\n\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "il dataset ha 17898 record e 9 features\n",
            "\n",
            "Numero di valori null per ogni feature:\n",
            " Mean of the integrated profile                  0\n",
            " Standard deviation of the integrated profile    0\n",
            " Excess kurtosis of the integrated profile       0\n",
            " Skewness of the integrated profile              0\n",
            " Mean of the DM-SNR curve                        0\n",
            " Standard deviation of the DM-SNR curve          0\n",
            " Excess kurtosis of the DM-SNR curve             0\n",
            " Skewness of the DM-SNR curve                    0\n",
            "target_class                                     0\n",
            "dtype: int64\n",
            "\n",
            "Da quel che si può vedere, non ci sono valori nulli all'interno del nostro dataset. Nel caso in cui ne avessimo trovato uno avremo avuto due possibilità:\n",
            "\n",
            "- Se i valori nulli non sono molti possiamo procedere eliminando i record che riportano il valore nullo;\n",
            "- Se il numero di questi valori nulli è elevato si può procedere alla sostituzione con dei valori di default\n",
            "\n",
            "ci sono 2 classi differenti.\n",
            "0 -> non è una pulsar\n",
            "1 -> è una pulsar\n",
            "numero di record per ogni classe:\n",
            "0    16259\n",
            "1     1639\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ah__nVazvgK"
      },
      "source": [
        "La prima analisi che si dovrà effettuare è quella che permette di verificare i tipi di dati del dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "YmLz7Zwqt6X-",
        "outputId": "64b1bfcc-2d15-4d91-b2ba-e0ab992ff21a"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Mean of the integrated profile</th>\n",
              "      <th>Standard deviation of the integrated profile</th>\n",
              "      <th>Excess kurtosis of the integrated profile</th>\n",
              "      <th>Skewness of the integrated profile</th>\n",
              "      <th>Mean of the DM-SNR curve</th>\n",
              "      <th>Standard deviation of the DM-SNR curve</th>\n",
              "      <th>Excess kurtosis of the DM-SNR curve</th>\n",
              "      <th>Skewness of the DM-SNR curve</th>\n",
              "      <th>target_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>140.562500</td>\n",
              "      <td>55.683781</td>\n",
              "      <td>-0.234571</td>\n",
              "      <td>-0.699648</td>\n",
              "      <td>3.199833</td>\n",
              "      <td>19.110426</td>\n",
              "      <td>7.975532</td>\n",
              "      <td>74.242226</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>102.507812</td>\n",
              "      <td>58.882431</td>\n",
              "      <td>0.465318</td>\n",
              "      <td>-0.515088</td>\n",
              "      <td>1.677258</td>\n",
              "      <td>14.860146</td>\n",
              "      <td>10.576487</td>\n",
              "      <td>127.393578</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>103.015625</td>\n",
              "      <td>39.341648</td>\n",
              "      <td>0.323328</td>\n",
              "      <td>1.051164</td>\n",
              "      <td>3.121238</td>\n",
              "      <td>21.744669</td>\n",
              "      <td>7.735822</td>\n",
              "      <td>63.171909</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>136.750000</td>\n",
              "      <td>57.178448</td>\n",
              "      <td>-0.068415</td>\n",
              "      <td>-0.636238</td>\n",
              "      <td>3.642977</td>\n",
              "      <td>20.959280</td>\n",
              "      <td>6.896499</td>\n",
              "      <td>53.593662</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>88.726562</td>\n",
              "      <td>40.672226</td>\n",
              "      <td>0.600866</td>\n",
              "      <td>1.123492</td>\n",
              "      <td>1.178930</td>\n",
              "      <td>11.468719</td>\n",
              "      <td>14.269573</td>\n",
              "      <td>252.567307</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Mean of the integrated profile  ...  target_class\n",
              "0                       140.562500  ...             0\n",
              "1                       102.507812  ...             0\n",
              "2                       103.015625  ...             0\n",
              "3                       136.750000  ...             0\n",
              "4                        88.726562  ...             0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttDLgb2U0vmh"
      },
      "source": [
        "Possiamo notare che tutte le features tranne la classe sono reali, perchè sono il prodotto di calcoli statistici su dei fenomeni fisici e quindi intrinsecamente continui.\n",
        "La classe di appartenenza invece è discreta, e visto che può assumere due valori possiamo dichiarare che il nostro problema consiste in una classificazione di tipo binario.\n",
        "\n",
        "Un'altra analisi preliminare che possiamo eseguire consiste nell'analizzare dei dati statistici relativi ai nostri campi del dataset. Questa analisi estrapolerà questi valori:\n",
        "\n",
        "- **count:** specifica il numero dei record presenti nel dataset\n",
        "- **mean:** specifica la media dell'attributo calcolata per tutti i record\n",
        "- **std:** specifica la deviazione standard dell'attributo\n",
        "- **min:** specifica il valore minimo dell'attributo\n",
        "- **25%:** il 25% dei record ha un valore minore di quello visualizzato (lower percentile)\n",
        "- **50%:** il 50% dei record ha un valore minore di quello visualizzato (median percentile)\n",
        "- **75%:** il 75% dei record ha un valore minore di quello visualizzato (upper percentile)\n",
        "- **max:** specifica il valore massimo dell'attributo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "zbrmcfcR2oSQ",
        "outputId": "205ae5c9-6261-4f99-a536-ca1d380c640c"
      },
      "source": [
        "dataset.describe()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Mean of the integrated profile</th>\n",
              "      <th>Standard deviation of the integrated profile</th>\n",
              "      <th>Excess kurtosis of the integrated profile</th>\n",
              "      <th>Skewness of the integrated profile</th>\n",
              "      <th>Mean of the DM-SNR curve</th>\n",
              "      <th>Standard deviation of the DM-SNR curve</th>\n",
              "      <th>Excess kurtosis of the DM-SNR curve</th>\n",
              "      <th>Skewness of the DM-SNR curve</th>\n",
              "      <th>target_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>17898.000000</td>\n",
              "      <td>17898.000000</td>\n",
              "      <td>17898.000000</td>\n",
              "      <td>17898.000000</td>\n",
              "      <td>17898.000000</td>\n",
              "      <td>17898.000000</td>\n",
              "      <td>17898.000000</td>\n",
              "      <td>17898.000000</td>\n",
              "      <td>17898.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>111.079773</td>\n",
              "      <td>46.549370</td>\n",
              "      <td>0.477856</td>\n",
              "      <td>1.770286</td>\n",
              "      <td>12.614424</td>\n",
              "      <td>26.326546</td>\n",
              "      <td>8.303542</td>\n",
              "      <td>104.857712</td>\n",
              "      <td>0.091574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>25.652920</td>\n",
              "      <td>6.843179</td>\n",
              "      <td>1.064041</td>\n",
              "      <td>6.167913</td>\n",
              "      <td>29.472887</td>\n",
              "      <td>19.470539</td>\n",
              "      <td>4.506091</td>\n",
              "      <td>106.514359</td>\n",
              "      <td>0.288432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>5.812500</td>\n",
              "      <td>24.772041</td>\n",
              "      <td>-1.876011</td>\n",
              "      <td>-1.791886</td>\n",
              "      <td>0.213211</td>\n",
              "      <td>7.370432</td>\n",
              "      <td>-3.139270</td>\n",
              "      <td>-1.976976</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>100.929688</td>\n",
              "      <td>42.376019</td>\n",
              "      <td>0.027098</td>\n",
              "      <td>-0.188572</td>\n",
              "      <td>1.923077</td>\n",
              "      <td>14.437331</td>\n",
              "      <td>5.781506</td>\n",
              "      <td>34.960505</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>115.078125</td>\n",
              "      <td>46.947479</td>\n",
              "      <td>0.223240</td>\n",
              "      <td>0.198710</td>\n",
              "      <td>2.801839</td>\n",
              "      <td>18.461315</td>\n",
              "      <td>8.433515</td>\n",
              "      <td>83.064556</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>127.085938</td>\n",
              "      <td>51.023203</td>\n",
              "      <td>0.473325</td>\n",
              "      <td>0.927783</td>\n",
              "      <td>5.464256</td>\n",
              "      <td>28.428104</td>\n",
              "      <td>10.702959</td>\n",
              "      <td>139.309326</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>192.617188</td>\n",
              "      <td>98.778908</td>\n",
              "      <td>8.069522</td>\n",
              "      <td>68.101624</td>\n",
              "      <td>223.392136</td>\n",
              "      <td>110.642212</td>\n",
              "      <td>34.539845</td>\n",
              "      <td>1191.000854</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Mean of the integrated profile  ...  target_class\n",
              "count                     17898.000000  ...  17898.000000\n",
              "mean                        111.079773  ...      0.091574\n",
              "std                          25.652920  ...      0.288432\n",
              "min                           5.812500  ...      0.000000\n",
              "25%                         100.929688  ...      0.000000\n",
              "50%                         115.078125  ...      0.000000\n",
              "75%                         127.085938  ...      0.000000\n",
              "max                         192.617188  ...      1.000000\n",
              "\n",
              "[8 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX0SCNL86PXg"
      },
      "source": [
        "###Grafici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5fam4xx-XnL"
      },
      "source": [
        "####Istogramma"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "89BQ9U4VtsmL",
        "outputId": "881da7d9-e252-46b5-dfdc-681ef4571b6f"
      },
      "source": [
        "feature_histogram(dataset = dataset, title = \"Distribuzione delle classi\", feature = \"target_class\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"525px\"\n",
              "            src=\"https://plotly.com/~freguti/15.embed\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fa4da72ec10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEwUWW8m3FS-"
      },
      "source": [
        "Da quel che possiamo vedere il dataset che abbiamo è molto sbilanciato. Per ogni record di classe \"1\" esistono 10 record di classe \"0\".\n",
        "\n",
        "Questo sbilanciamento potrebbe portare il modello predittivo ad avere dei bias, portandolo a predirre più frequentemente la classe con più campioni solamente perchè è stata più presente in fase di train.\n",
        "\n",
        "Gli Istogrammi sono molto utili per vedere la distribuzione delle classi per ogni feature in relazione al suo valore.\n",
        "Tramite uno slider è possibile visualizzare delle features diverse."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "jJhTutRh0s4L",
        "outputId": "792f6e65-7262-4a80-cac0-de42f052a05c"
      },
      "source": [
        "pulsar = dataset[dataset['target_class'] == 1]\n",
        "not_pulsar = dataset[dataset['target_class'] == 0]\n",
        "\n",
        "create_slider_controlled_histogram(dataset.columns,pulsar,not_pulsar, \"mean\",'bar_mean')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"525px\"\n",
              "            src=\"https://plotly.com/~freguti/419.embed\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fa4db093590>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eqc_yIJGdxD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "outputId": "3fd36eed-3a2f-48bf-d5ce-15bc63ee1efc"
      },
      "source": [
        "create_slider_controlled_histogram(dataset.columns,pulsar,not_pulsar,\"deviation\",'bar_deviation')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"525px\"\n",
              "            src=\"https://plotly.com/~freguti/421.embed\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fa4d9f74350>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "jJ3qWR0nGVRX",
        "outputId": "755ba8f3-aaf7-4378-ccd6-edf379dbc797"
      },
      "source": [
        "create_slider_controlled_histogram(dataset.columns,pulsar,not_pulsar,\"kurtosis\",'bar_kurtosis')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"525px\"\n",
              "            src=\"https://plotly.com/~freguti/423.embed\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fa4dade8a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "2EJ7rATXcW5T",
        "outputId": "d939e5bb-2db9-460d-ff71-f4f18fb4df84"
      },
      "source": [
        "create_slider_controlled_histogram(dataset.columns,pulsar,not_pulsar,\"skewness\",'bar_skewness')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"525px\"\n",
              "            src=\"https://plotly.com/~freguti/425.embed\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fa4da44c910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56fdYyeMy1Hy"
      },
      "source": [
        "A causa di un limite dello strumento, che consente un upload limitato di dati, ho dovuto dividere il grafico in 4. Ho optato per separare i grafici per metodo statistico applicato alle due misurazioni.\n",
        "\n",
        "ho usato una normalizzazione percentuale che mi mostra, per ogni classe, la distribuzione percentuale dei valori. L'ho fatto a causa dell'elevata differenza numerica tra le due classi, che non mi permetteva di mostrare correttamente i valori assoluti.\n",
        "\n",
        "Si può notare che la distribuzione dei valori \"non pulsar\" tende ad essere la maggior parte delle volte una distribuzione Gaussiana o di Poisson.\n",
        "\n",
        "La distribuzione dei valori \"pulsar\" tende ad essere più costante e disordinata, evitando una concentrazione di valori. Questo deriva dalle nostre supposizioni iniziali, che indicavano come univoci i pattern di ogni pulsar.\n",
        "\n",
        "Questa differenza può essere significativa in fase di classificazione, perchè le varie features tendono a sovrapporsi poche volte, ciò implica che potenzialmente tutte le classi possono essere utili per determinare la classe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8Pl9OFp9-K5"
      },
      "source": [
        "####Boxplot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "XcTcYxCTNAMR",
        "outputId": "d11b44fd-1576-4324-8e66-d6ecd95f9256"
      },
      "source": [
        "create_slider_controlled_Boxplot(dataset.columns,pulsar,not_pulsar, \"mean\",'box_mean')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"525px\"\n",
              "            src=\"https://plotly.com/~freguti/427.embed\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fa4dafbc510>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "gGo-h1rfQx2Q",
        "outputId": "b865e799-4885-4c28-ae61-f064343f457d"
      },
      "source": [
        "create_slider_controlled_Boxplot(dataset.columns,pulsar,not_pulsar, \"deviation\",'box_deviation')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"525px\"\n",
              "            src=\"https://plotly.com/~freguti/429.embed\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fa4dac6e890>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "kMmOM1dIQyOM",
        "outputId": "7523e3a9-0a03-4960-8a8e-ccfd5c5920ed"
      },
      "source": [
        "create_slider_controlled_Boxplot(dataset.columns,pulsar,not_pulsar, \"kurtosis\",'box_kurtosis')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"525px\"\n",
              "            src=\"https://plotly.com/~freguti/431.embed\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fa4da63ad50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "sMQTI5QonXW0",
        "outputId": "a46c588b-c2c7-4ef4-896c-6078e5e84b2e"
      },
      "source": [
        "create_slider_controlled_Boxplot(dataset.columns,pulsar,not_pulsar, \"skewness\",'box_skewness')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"525px\"\n",
              "            src=\"https://plotly.com/~freguti/433.embed\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fa4da2ae210>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8wxYPlhRJxI"
      },
      "source": [
        "Visto che nuessuun parametro ha una distribuzione media troppo simile alla controparte della classe opposta, possiamo presupporre che siano tutti rilevanti in fase di classificazione."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKkE3YA--UeL"
      },
      "source": [
        "####Matrice di correlazione"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXUhKSZWsJ2v"
      },
      "source": [
        "Una matrice di correlazione è una tabella che mostra i coefficienti di correlazione tra insiemi di variabili. Ogni variabile casuale ( $X_i$ ) nella tabella è correlata con ciascuno degli altri valori nella tabella ( $X_j$ ); questo permette di vedere quali coppie hanno la più alta correlazione. La correlazione si riferisce a qualsiasi associazione statistica, ma nell'uso comune del termine si indica quanto due variabili siano vicine ad avere una relazione lineare l'una con l'altra.\n",
        "\n",
        "Nel grafico sottostante abbiamo utilizzato la correlazione di Pearson, che misura la correlazione lineare tra due variabili statistiche X e Y, ritornando un valore compreso tra -1 e 1.\n",
        "- Se il valore è vicino a -1 vuol dire che le due variabili hanno una forte correlazione negativa, cioè al crescere della prima decresce la seconda;\n",
        "- Se il valore è vicino a 1 vuol dire che le due variabili hanno una forte correlazione, cioè al crescere della prima crescerà anche la seconda;\n",
        "- Se il valore è vicino allo 0 le due variabili sono scorrelate\n",
        "\n",
        "Questo valore viene calcolato così: $Corr_{i,j} = \\frac{cov(X,Y)}{\\sigma_x\\sigma_y}$\n",
        "\n",
        "dove cov è la covarianza tra i due valori e $\\sigma$ è lo scarto quadratico medio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "t0gFY-ChbLHf",
        "outputId": "e81574d9-863c-465a-9688-0101fbdbc230"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "corr = dataset.corr(method='pearson')\n",
        "#corr.style.background_gradient(cmap='coolwarm').set_precision(2)\n",
        "title = \"Correlation Matrix\"\n",
        "\n",
        "z_text = np.around(corr.values.tolist(), decimals=2)\n",
        "\n",
        "figure = ff.create_annotated_heatmap(z=corr.values, \n",
        "                                         x=corr.columns.tolist(), \n",
        "                                         y=corr.index.tolist(),\n",
        "                                         annotation_text=z_text,\n",
        "                                         colorscale=PALETTE_HEATMAP,\n",
        "                                         showscale=True)\n",
        "\n",
        "figure.layout.title = title\n",
        "figure.layout.autosize = False\n",
        "figure.layout.width = 850\n",
        "figure.layout.height = 850\n",
        "figure.layout.margin = go.layout.Margin(l=140, r=100, b=200, t=80)\n",
        "figure.layout.xaxis.update(side='bottom')\n",
        "figure.layout.yaxis.update(side='left')\n",
        "\n",
        "for i in range(len(figure.layout.annotations)):\n",
        "    figure.layout.annotations[i].font.size = 8\n",
        "                                \n",
        "py.iplot(figure, filename=title)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"850px\"\n",
              "            height=\"850px\"\n",
              "            src=\"https://plotly.com/~freguti/106.embed\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fa4da63ab90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMGCos5csnL6"
      },
      "source": [
        "Possiamo notare che sia la skewness e la kurtosis (0,95 per l'integrated profile e 0,92 per la DM-SNR curve) che la mean e la standard deviation (0,55 per l'integrated profile e 0,80 per la DM-SNR curve) sono estremamente correlate tra loro.\n",
        "\n",
        "Essendo che i nostri parametri sono ricavati dall'applicazione di funzioni statistiche su due grandezze fisiche, era aspettato trovare due blocchi di forte correlazione all'interno della matrice.\n",
        "\n",
        "Possiamo notare come la kurtosis e la skewness of the integrated profile siano estremamente importanti per determinare la target_class, mentre la mean e la standard deviation della DM-SNR curve sono meno importanti, ma comunque molto rilevanti. Anche la mean of integrated profile ha una correlazione significativa (-0,68), cioè ha una forte correlazione inversa.\n",
        "\n",
        "Notiamo che nessun attributo ha un indice di correlazione con la target_class vicino allo 0, ciò significa che sono tutti importanti nella determinazione della classe. Possiamo notare che comunque esistono 3 attributi che hanno un indice di correlazione molto alto ($|M_{i,j}|>= 0,80$) tra loro, quindi possiamo intuire che qualche feature potrebbe essere eliminata."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTjBRUgS-l4h"
      },
      "source": [
        "####Dendrogramma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHPVEOyBC5mY"
      },
      "source": [
        "Un dendrogramma è un grafo ad albero utilizzato per raffigurare la disposizione dei cluster relativi alle features. L'obiettivo è quello di visualizzare somiglianze tra i cluster, rilevando potenziali caratteristiche duplicate e potenzialmente aiutando nella riduzione della dimensionalità.\n",
        "\n",
        "Questo grafico indica la forza delle relazioni esistenti tra due cluster in base alla distanza che intercorre tra l'origine e la linea verticale più vicina che connette le linee orizzontali corrispondenti ai due elementi considerati."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "uDOQAvNIDazv",
        "outputId": "06e827b2-9a54-47ff-9d44-c494268cb0d5"
      },
      "source": [
        "names = dataset.columns\n",
        "inv_corr = 1 - corr # This is the 'dissimilarity' method\n",
        "\n",
        "fig = ff.create_dendrogram(inv_corr, \n",
        "                           labels=names, \n",
        "                           colorscale=COLOR_PALETTE,\n",
        "                           linkagefun=lambda x: hc.linkage(x, 'average'))\n",
        "\n",
        "fig['layout'].update(dict(\n",
        "    title=\"Dendrogramma di correlazione tra gli attributi\",\n",
        "    width=800, \n",
        "    height=600,\n",
        "    xaxis=dict(\n",
        "        title='Features',\n",
        "    ),\n",
        "    yaxis=dict(\n",
        "        title='Distance',\n",
        "    ),\n",
        "))\n",
        "py.iplot(fig, filename='dendrogram_corr_clustering')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"800px\"\n",
              "            height=\"600px\"\n",
              "            src=\"https://plotly.com/~freguti/104.embed\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fa4d9c35ad0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-I50cHLHh4n"
      },
      "source": [
        "Possiamo notare che la skewness of the integrated profile e l'excess kurtosis of the integrated profile hanno una distanza molto minore rispetto agli altri cluster. Possiamo dedurre che molto probabilmente è possibile effettuare una semplificazione al dataset, riducendone la dimensionalità."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_dUfjXNJ9Oj"
      },
      "source": [
        "###Suddivisione del dataset e normalizzazione dei dati\n",
        "\n",
        "Per evitare \"data leakage\" è importante eseguire ogni trasformazione dopo la separazione dei dati. Il test set deve simulare dei dati reali acquisiti dopo l'addestramento del nostro modello, quindi non può essere utilizzato in nessun caso insieme al train set.\n",
        "\n",
        "Molte volte i dataset contengono dei dati che variano molto rispetto alla media in termini di grandezza. Molti algoritmi di classificazione utilizzano la distanza euclidea per misurare la distanza tra due dati, quindi delle distanze troppo grandi possono causare dei problemi.\n",
        "\n",
        "Per questo motivo, due features diverse possono influenzare in maniera diversa il classificatore in base all'unità di misura. Per esempio una features i cui valori sono dell'ordine delle migliaia (900, 5000, 2000) avrà un peso maggiore di una feature con valori nell'ordine delle unità (0,1, 10, 100) anche se in percentuale la variazione della prima risulterebbe essere minore rispetto alla seconda.\n",
        "\n",
        "Per ovviare a questo problema utilizzeremo lo StandardScaler, che normalizzerà i nostri dati sottraendogli la media e dividendoli per lo scarto quadratico medio:\n",
        "$x' = \\frac{x - \\mu_x}{\\sigma_x}$\n",
        "\n",
        "La normalizzazione verrà eseguita dopo la suddivisione dei dati, perchè il test set non deve essere influenzato in alcun modo dal train set e, eseguendo la normalizzazione sull'intero dataset, lo influenzerei tramite la media e la varianza del dataset intero.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYgY4lbSKJHL"
      },
      "source": [
        "def normalize(X_set):\n",
        "  scaler = StandardScaler(with_mean=True, with_std=True, copy=True)\n",
        "  return scaler.fit_transform(X_set)\n",
        "\n",
        "X_dataset = dataset.drop(['target_class'], axis=1)\n",
        "y_data = dataset['target_class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_dataset,y_data, test_size=0.2, random_state=42, stratify=y_data)\n",
        "X_train_norm = normalize(X_train)\n",
        "X_test_norm = normalize(X_test)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ks_p_6K73NOl"
      },
      "source": [
        "<a name=\"PCA\"></a>\n",
        "##Principal Component Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kJXZvriSX7f"
      },
      "source": [
        "La Principal Component Analysis (PCA) è una tecnica che consente di ridurre la dimensionalità del dataset, proiettando negli assi principali gli attributi con maggior varianza.\n",
        "La variazione di complessità avviene limitandosi ad analizzare le variabili con maggior varianza, quindi, più importanti.\n",
        "\n",
        "Essendo che le componenti vengono inserite man mano in nuovi assi, si otterrà una base ortogonale nella quale tutte le componenti aggiunte saranno a loro volta ortogonali alle precedenti.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "9weE_fxAK-Rv",
        "outputId": "f4d2e65f-48da-4210-d28d-43cb3c6eb940"
      },
      "source": [
        "p, x_train_PCA6 = PCA_Reduction(X_train_norm, plot = True , n_components = 6)\n",
        "x_train_PCA4 = PCA_Reduction(X_train_norm, plot = False , n_components = 4)\n",
        "p"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"525px\"\n",
              "            src=\"https://plotly.com/~freguti/122.embed\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fa4da6e7650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "C25h0FiHaC4W",
        "outputId": "b320c08d-1371-4428-ef1d-5ca0939fd53e"
      },
      "source": [
        "x_train_PCA6.head(5)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PC#1</th>\n",
              "      <th>PC#2</th>\n",
              "      <th>PC#3</th>\n",
              "      <th>PC#4</th>\n",
              "      <th>PC#5</th>\n",
              "      <th>PC#6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-2.463495</td>\n",
              "      <td>-2.561296</td>\n",
              "      <td>0.358478</td>\n",
              "      <td>-0.816815</td>\n",
              "      <td>4.594153</td>\n",
              "      <td>-0.234365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.221554</td>\n",
              "      <td>-0.398319</td>\n",
              "      <td>0.064316</td>\n",
              "      <td>0.146490</td>\n",
              "      <td>-0.779138</td>\n",
              "      <td>-0.105041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.568718</td>\n",
              "      <td>0.347144</td>\n",
              "      <td>0.085768</td>\n",
              "      <td>-0.013019</td>\n",
              "      <td>-0.853417</td>\n",
              "      <td>0.125864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.767799</td>\n",
              "      <td>-0.632460</td>\n",
              "      <td>-0.086334</td>\n",
              "      <td>0.430180</td>\n",
              "      <td>0.717322</td>\n",
              "      <td>-0.516805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-2.056926</td>\n",
              "      <td>-1.624101</td>\n",
              "      <td>-0.212691</td>\n",
              "      <td>-0.535097</td>\n",
              "      <td>3.287983</td>\n",
              "      <td>0.109454</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       PC#1      PC#2      PC#3      PC#4      PC#5      PC#6\n",
              "0 -2.463495 -2.561296  0.358478 -0.816815  4.594153 -0.234365\n",
              "1 -0.221554 -0.398319  0.064316  0.146490 -0.779138 -0.105041\n",
              "2  0.568718  0.347144  0.085768 -0.013019 -0.853417  0.125864\n",
              "3 -0.767799 -0.632460 -0.086334  0.430180  0.717322 -0.516805\n",
              "4 -2.056926 -1.624101 -0.212691 -0.535097  3.287983  0.109454"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T6cSd_GQqyk"
      },
      "source": [
        "Quello che possiamo leggere da questo grafico è che già con le prime 4 componenti principali raggiungiamo una varianza cumulativa del 94,35%, mentre mantenendo le prime 6 otterremo una varianza del 99,55%.\n",
        "\n",
        "Possiamo ridurre la complessità del nostro dataset utilizzando solo le prime 4 componenti principali oppure mantenendo le prime 6. In entrambi i casi la varianza rimarrebbe molto alta, quindi bisognerà controllare quale delle due strategie porterebbe più benefici.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbowA3FtBA8v"
      },
      "source": [
        "<a name=\"bilanciamento\"></a>\n",
        "##Bilanciamento Dataset \n",
        "\n",
        "Esistono diverse tecniche per bilanciare il dataset:\n",
        "- **oversampling:** tecnica che consente di creare dei campioni appartenenti alla classe con meno record;\n",
        "- **undersampling:** tecnica che consente di rimuovere dei campioni appartenenti alla classe con più record;\n",
        "- **class weight:** tecnica che consente di bilanciare il dataset inserendo un parametro chiamato \"weight\" all'interno del dataset, che contiene i pesi delle varie classi del dataset. Il peso viene assegnato in base alla proporzione tra il numero di parametri della classe più grande e il numero di parametri della classe più piccola. (per esempio W = 1 per la classe \"0\" e W = 10 per la classe \"1\") \n",
        "Durante la fase di train i vari record influenzeranno il modello in proporzione al peso assegnatogli.\n",
        "Questa è la soluzione meno invasiva, perché non modifica il dataset ma solamente il modo in cui i record agiscono sul modello.\n",
        "\n",
        "Per bilanciare questo dataset ho optato per inserire in una pipeline prima un ridimensionamento della classe piccola tramite SMOTE, e a seguire una riduzione dei campioni appartenenti alla classe maggiore tramite un random undersampling.\n",
        "\n",
        "Il bilanciamento si deve eseguire solo sul train set, perché il test set deve essere più vicino possibile ai dati reali.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-uMg0xR6fBX",
        "outputId": "57dec612-94e0-48ab-961f-ee79be5c33d1"
      },
      "source": [
        "x = x_train_PCA4\n",
        "y = y_train\n",
        "\n",
        "model = KNeighborsClassifier()\n",
        "scores = cross_val_score(model, x, y, scoring='roc_auc', cv=5, n_jobs=-1)\n",
        "score = mean(scores)\n",
        "print('Lo score calcolato sul dataset a cui è stata applicata la PCA mantenendo 4 componenti è:%.3f' % (score))\n",
        "\n",
        "x = x_train_PCA6\n",
        "model = KNeighborsClassifier()\n",
        "scores = cross_val_score(model, x, y, scoring='roc_auc', cv=5, n_jobs=-1)\n",
        "score = mean(scores)\n",
        "print('Lo score calcolato sul dataset a cui è stata applicata la PCA mantenendo 6 componenti è:%.3f' % (score))\n",
        "\n",
        "x = X_train_norm\n",
        "model = KNeighborsClassifier()\n",
        "scores = cross_val_score(model, x, y, scoring='roc_auc', cv=5, n_jobs=-1)\n",
        "score = mean(scores)\n",
        "print('Lo score calcolato sul dataset normalizzato è:%.3f' % (score))\n",
        "\n",
        "x = x_train_PCA6"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lo score calcolato sul dataset a cui è stata applicata la PCA mantenendo 4 componenti è:0.923\n",
            "Lo score calcolato sul dataset a cui è stata applicata la PCA mantenendo 6 componenti è:0.950\n",
            "Lo score calcolato sul dataset normalizzato è:0.951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVz1RvYe7i1-"
      },
      "source": [
        "Guardando il rapporto prestazioni/riduzione della complessità possiamo assumere che la soluzione sia applicare la PCA e tenendo le prime 6 componenti principali.\n",
        "\n",
        "Facendo ciò riusciamo a ridurre la complessità senza perdere risoluzione, perché le features rimosse hanno un peso sulla varianza cumulativa dello 0,45%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEl3FLas68f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e01afbc2-ad0f-4399-fb2e-2030db6ed89e"
      },
      "source": [
        "#Bilanciamento dataset. da fare solo sul training set\n",
        "over_ratio = [0.3,0.4,0.5]\n",
        "under_ratio = [0.7,0.6,0.5,1]\n",
        "hi_score = 0\n",
        "best_ratio = (0,0)\n",
        "for o in over_ratio:\n",
        "  for u in under_ratio:\n",
        "    model = KNeighborsClassifier()#SVC(kernel='linear', probability=True, random_state=42)\n",
        "    over = SMOTE(sampling_strategy=o)\n",
        "    under = RandomUnderSampler(sampling_strategy=u)\n",
        "    steps = [('over', over), ('under', under), ('model', model)]\n",
        "    pipeline = Pipeline(steps=steps)\n",
        "    # evaluate pipeline\n",
        "    scores = cross_val_score(pipeline, x, y, scoring='roc_auc', cv=5, n_jobs=-1)\n",
        "    score = mean(scores)\n",
        "    if score > hi_score:\n",
        "      hi_score = score\n",
        "      best_ratio = (o,u)\n",
        "      print(\"cambio ratio {0} {1} \".format(o,u))\n",
        "    print('SMOTE oversampling rate:%.1f, Random undersampling rate:%.1f , Media ROC AUC: %.3f' % (o, u, score))\n",
        "print(\"Il ratio che restituisce un punteggio maggiore è (oversampling: {0},undersampling: {1})\".format(best_ratio[0],best_ratio[1]))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cambio ratio 0.3 0.7 \n",
            "SMOTE oversampling rate:0.3, Random undersampling rate:0.7 , Media ROC AUC: 0.954\n",
            "SMOTE oversampling rate:0.3, Random undersampling rate:0.6 , Media ROC AUC: 0.953\n",
            "SMOTE oversampling rate:0.3, Random undersampling rate:0.5 , Media ROC AUC: 0.953\n",
            "SMOTE oversampling rate:0.3, Random undersampling rate:1.0 , Media ROC AUC: 0.954\n",
            "cambio ratio 0.4 0.7 \n",
            "SMOTE oversampling rate:0.4, Random undersampling rate:0.7 , Media ROC AUC: 0.954\n",
            "SMOTE oversampling rate:0.4, Random undersampling rate:0.6 , Media ROC AUC: 0.951\n",
            "SMOTE oversampling rate:0.4, Random undersampling rate:0.5 , Media ROC AUC: 0.951\n",
            "SMOTE oversampling rate:0.4, Random undersampling rate:1.0 , Media ROC AUC: 0.952\n",
            "SMOTE oversampling rate:0.5, Random undersampling rate:0.7 , Media ROC AUC: 0.950\n",
            "SMOTE oversampling rate:0.5, Random undersampling rate:0.6 , Media ROC AUC: 0.949\n",
            "SMOTE oversampling rate:0.5, Random undersampling rate:0.5 , Media ROC AUC: 0.951\n",
            "cambio ratio 0.5 1 \n",
            "SMOTE oversampling rate:0.5, Random undersampling rate:1.0 , Media ROC AUC: 0.954\n",
            "Il ratio che restituisce un punteggio maggiore è (oversampling: 0.5,undersampling: 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_bhwb39L4MI"
      },
      "source": [
        "- **SMOTE** (Synthetic Minority Oversampling Technique), è una tecnica che seleziona due campioni appartenenti ad una classe che sono vicini tra loro, \"traccia una linea\" tra loro nello spazio delle features e crea un nuovo campione lungo questa linea.\n",
        "\n",
        "- **Random Undersampling:** Random undersampling è una tecnica che consente semplicemente nel rimuovere dei campioni casuali della classe più popolosa.\n",
        "\n",
        "- **Pipeline:** Tramite la pipeline è possibile effettuare in parallelo l'aumento di campione di una classe tramite SMOTE e la riduzione dell'altra classe tramite random undersampling.\n",
        "\n",
        "- **ROC AUC Score:** Una ROC è una curva che mostra le performance del classificatore, tenendo conto del *True Positive Rate* e del *False Positive Rate*. AUC, \"Area under the ROC Curve.\", misura l'area sotto la ROC curve. Questo punteggio è quindi calcolato applicando la tecnica di AUC sulla ROC.   \n",
        "\n",
        "- **SVC:** SVC (Support Vector Clustering) è un modello di apprendimento utilizzato per la classificazione, che estende il più famoso SVM (Support Vector Machine).\n",
        "\n",
        "Il punteggo relativo ai differenti ratio è molto simile tra loro, compreso il punteggio calcolato con il ratio originale. La scelta di uno rispetto all'altro non è troppo rilevante, quindi ho deciso di utilizzare il ratio (0.3, 1) perché mediamente ha performato leggermente meglio degli altri.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S2kOCfG3x2j"
      },
      "source": [
        "<a name=\"classificazione\"></a>\n",
        "##Classificazione"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3GkWRnjrDtV"
      },
      "source": [
        "In questa ultima parte proveremo vari modelli di classificazione sul nostro dataset per vedere quale si comporta meglio sui nostri dati.\n",
        "\n",
        "Un problema di classificazione consiste nel ricavare la probabilità $P(y|x,\\theta)$, dove $\\theta$ descrive i parametri del nostro modello."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NhI_41g45qR"
      },
      "source": [
        "kf = StratifiedKFold(n_splits=5, random_state=42, shuffle = True)\n",
        "\n",
        "lr = LogisticRegression(random_state=42,max_iter = 20000)\n",
        "svm = SVC(probability=True, random_state=42)\n",
        "nbc = GaussianNB()\n",
        "rfc = RandomForestClassifier(random_state=42)\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "x_test_PCA6 = PCA_Reduction(X_test_norm, plot = False , n_components = 6)\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7RbubBTgsCg"
      },
      "source": [
        "<a name=\"lr\"></a>\n",
        "###Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esshrKjEy9BT"
      },
      "source": [
        "Il primo classificatore che analizzeremo è la logistic regression, che consiste in un modello non lineare che utilizza una funzione Sigmoid per classificare i campioni.\n",
        "\n",
        "In alcuni casi, possiamo usare la linear regression (la quale non prova a predirre la classe ma il valore esatto partendo da un dato input x e calcolando un output y) per determinare un boundary appropriato. Tuttavia, poichè l'output è generalmente binario o discreto, esistono metodi di regressione più efficienti.\n",
        "\n",
        "Durante la classificazione mediante la regressione, i nostri parametri $\\theta$ corrispondono i coefficienti $w$ del nostro modello.\n",
        "\n",
        "$P(y = 0 | X,\\theta) = g(w^TX) = \\frac{1}{1+e^{w^TX}}$\n",
        "$P(y = 1 | X,\\theta) = 1 - g(w^TX) = \\frac{e^{w^TX}}{1+e^{w^TX}}$\n",
        "\n",
        "I parametri vengono ricavati in maniera analoga rispetto agli altri problemi di regressione. Bisogna cercare la Maximum Likelihood Estimation per $w$. La probabilità dei dati forniti dal modello è:\n",
        "\n",
        "$L(y|x,w) = \\prod_{i}{(1 - g(x_i,w))^{y_i}.g(x_i,w)^{1-y_i}}$\n",
        "\n",
        "Applicando delle funzioni logaritmiche a questi risultati possiamo ottenere una funzione non lineare concava.\n",
        "\n",
        "Questo modello, rispetto alla regressione lineare, può modellare meglio la zona compresa tra 0 e 1. Per conoscere i pesi, bisogna calcolare la MLE ed applicare l'algoritmo di gradient descent fino alla convergenza del valore di accuratezza.\n",
        "\n",
        "Questi algoritmi sono:\n",
        "- liblinear: è il migliore quando applicato ad un dataset piccolo\n",
        "- C: valore compreso tra 0,01 e 100. Più è piccolo il valore, maggiore è la regolarizzazione\n",
        "- penalty: penalità \"l1\" e \"l2\" per la regolarizzazione\n",
        "  - l1, che penalizza ogni errore allo stesso modo $S=∑^n_{i=1}|yi−f(xi)|$\n",
        "  - l2, che penalizza maggiormente valori più grandi $S=∑^n_{i=1}(yi−f(xi))^2$\n",
        "\n",
        "dove $y_i$ è la vera label e $f(x_i)$ è la label che viene assegnata\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_Sl6mLvgaqD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "outputId": "d8f9ec6b-8561-4313-e559-db5b8557f3b7"
      },
      "source": [
        "print(\"Simulazione eseguita con il dataset su cui è stata applicata la PCA e sono stati mantenute 6 componenti:\\n\")\n",
        "score6_lr = grid_search_cv(lr, LR_PARAM, x_train_PCA6, y_train, kf)\n",
        "print_best_scores(score6_lr, n=5)\n",
        "\n",
        "#print(\"Simulazione eseguita con il dataset su cui è stata applicata la PCA e sono stati mantenute 4 componenti:\\n\")\n",
        "#score4_lr = grid_search_cv(lr, LR_PARAM, x_train_PCA4, y_train, kf)\n",
        "#print_best_scores(score4_lr, n=5)\n",
        "\n",
        "#print(\"Simulazione eseguita con il dataset su cui non è stata applicata la PCA:\\n\")\n",
        "#score_lr = grid_search_cv(lr, LR_PARAM, X_train_norm, y_train, kf)\n",
        "#print_best_scores(score_lr, n=5)\n",
        "\n",
        "plot_learning_curve(score6_lr.best_estimator_, \"Curva di apprendimento di Logistic Regression (6 componenti analizzate)\", x_train_PCA6, y_train, cv=5,name=\"LR\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Simulazione eseguita con il dataset su cui è stata applicata la PCA e sono stati mantenute 6 componenti:\n",
            "\n",
            "Migliori parametri su insieme di validazione:\n",
            "+------------------+--------+--------------+-------------+\n",
            "|      Score       | clf__C | clf__penalty | clf__solver |\n",
            "+------------------+--------+--------------+-------------+\n",
            "| 0.836 (+/-0.029) |  0.1   |      l1      |  liblinear  |\n",
            "| 0.832 (+/-0.014) |  0.1   |      l2      |  newton-cg  |\n",
            "| 0.832 (+/-0.024) |   1    |      l1      |  liblinear  |\n",
            "| 0.829 (+/-0.017) |   1    |      l2      |  liblinear  |\n",
            "| 0.828 (+/-0.021) |   1    |      l2      |  newton-cg  |\n",
            "+------------------+--------+--------------+-------------+\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"525px\"\n",
              "            src=\"https://plotly.com/~freguti/438.embed\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fa4cdc33a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KuYhVNGrLaX"
      },
      "source": [
        "Da questa matrice possiamo notare come l'errore percentuale è abbastanza simile (il 15% delle volte assegnamo ad un elemento della calsse 0 la label 1 e il 19% delle volte l'opposto), mentre l'errore assoluto è nettamente diverso a causa della presenza di più valori \"0\" nel dataset di test. \n",
        "\n",
        "Questo problema non è possibile risolverlo con un bilanciamento del dataset perché, come detto in precedenza, deve essere il più fedele possibile ai dati reali che riceveremo in input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72qdnwT3IPvt",
        "outputId": "6c7dd572-70f0-407f-df34-cd15ab1c8f5c"
      },
      "source": [
        "print_confusion_matrix(score6_lr, x_test_PCA6, y_test)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+----------------------+------------------+\n",
            "|                 | Predicted not pulsar | Predicted pulsar |\n",
            "+-----------------+----------------------+------------------+\n",
            "| True not pulsar |         2687         |       565        |\n",
            "|   True pulsar   |          65          |       263        |\n",
            "+-----------------+----------------------+------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcsLAk7uhIE5"
      },
      "source": [
        "<a name=\"svm\"></a>\n",
        "###Support Vector Machine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSVFbaZoe8Q_"
      },
      "source": [
        "Una Support Vector Machine (SVM)  è un classificatore discriminativo definito formalmente da un iperpiano di separazione. In altre parole, dati dei labeled sample (quindi un supervised learning), l'output dell'algoritmo genera un iperpiano ottimale che classifica poi i nuovi esempi.\n",
        "\n",
        "SVM utilizza delle funzioni kernel per identificare degli elementi nello spazio senza l'ausilio di coordinate, ma semplicemente calcolando il prodotto interno delle immagini di tutte le coppie di dati nello spazio della funzione. Questa operazione risulta spesso computazionalmente più efficiente del calcolo delle coordinate e viene chiamata \"kernel trick\".\n",
        "\n",
        "Per selezionare l'iperpiano ottimale tra le infinite possibilità, SVM ne genera un certo numero e successivamente sceglie quello che ha la distanza maggiore tra sè e il punto più vicino di entrambe le classi. Fatto ciò viene creato un \"boundary\", e i punti che si trovano su di esso sono chiamati \"vettori di supporto\": non vengono utilizzati quindi tutti i dati categorizzati ma solo quelli sui margini.\n",
        "\n",
        "Dobbiamo trovare l'iperpiano che massimizza i margini, che in due dimensione è  definito come una retta che divide il piano in due parti, ognuna che specifica una delle due classi.\n",
        "\n",
        "I parametri principali sono:\n",
        "\n",
        "- linear: è la più semplice delle SVM. Trova l'iperpiano che separa nel migliore dei modi i nostri dati di training\n",
        "- C: il parametro C di SVM fornisce all'ottimizzatore una misura su quanto è importante evitare classificazioni errate. Più è alto il valore di C, più sarà piccolo il margine della retta se essa con tale margine è in grado di classificare meglio i nostri dati.\n",
        "- rbf: questo parametro indica che stiamo usando una Radial Basis Function kernel per effettuare il prodotto scalare\n",
        " - gamma: definisce fino a che punto il valore di un singolo elemento possa influire; se il valore di gamma è basso, significa molta influenza, con un valore alto, poca influenza.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfOnIFNuhK8B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "outputId": "b2380194-4b79-4f44-ebbf-1b4da13143c3"
      },
      "source": [
        "print(\"Simulazione eseguita con il dataset su cui è stata applicata la PCA e sono stati mantenute 6 componenti:\\n\")\n",
        "score6_svm = grid_search_cv(svm, SVM_PARAM, x_train_PCA6, y_train, kf)\n",
        "print_best_scores(score6_svm, n=5)\n",
        "\n",
        "#print(\"Simulazione eseguita con il dataset su cui è stata applicata la PCA e sono stati mantenute 4 componenti:\\n\")\n",
        "#score4_svm = grid_search_cv(svm, SVM_PARAM, x_train_PCA4, y_train, kf)\n",
        "#print_best_scores(score4_svm, n=5)\n",
        "\n",
        "#print(\"Simulazione eseguita con il dataset su cui non è stata applicata la PCA:\\n\")\n",
        "#score_svm = grid_search_cv(svm, SVM_PARAM, X_train_norm, y_train, kf)\n",
        "#print_best_scores(score_svm, n=5)\n",
        "\n",
        "plot_learning_curve(score6_svm.best_estimator_, \"Curva di apprendimento di Logistic Regression (6 componenti analizzate)\", x_train_PCA6, y_train, cv=5,name=\"SVM\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Simulazione eseguita con il dataset su cui è stata applicata la PCA e sono stati mantenute 6 componenti:\n",
            "\n",
            "Migliori parametri su insieme di validazione:\n",
            "+------------------+--------+------------+-------------+\n",
            "|      Score       | clf__C | clf__gamma | clf__kernel |\n",
            "+------------------+--------+------------+-------------+\n",
            "| 0.866 (+/-0.023) |   1    |    0.1     |     rbf     |\n",
            "| 0.860 (+/-0.025) |   10   |    0.1     |     rbf     |\n",
            "| 0.858 (+/-0.022) |  100   |    0.1     |     rbf     |\n",
            "| 0.855 (+/-0.018) |   1    |     1      |     rbf     |\n",
            "| 0.854 (+/-0.024) |  0.1   |   linear   |     None    |\n",
            "+------------------+--------+------------+-------------+\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"525px\"\n",
              "            src=\"https://plotly.com/~freguti/440.embed\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fa4ce050b50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMCCbalLIbPO",
        "outputId": "1fc5de93-6444-4944-d9ca-1d0003f017b3"
      },
      "source": [
        "print_confusion_matrix(score6_svm, x_test_PCA6, y_test)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+----------------------+------------------+\n",
            "|                 | Predicted not pulsar | Predicted pulsar |\n",
            "+-----------------+----------------------+------------------+\n",
            "| True not pulsar |         2887         |       365        |\n",
            "|   True pulsar   |         152          |       176        |\n",
            "+-----------------+----------------------+------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ1NhrxthSxZ"
      },
      "source": [
        "<a name=\"nbc\"></a>\n",
        "###Naïve Bayes Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwb1b1uZH_q-"
      },
      "source": [
        "Il Naïve Bayes Classifier è un algoritmo predittivo di tipo statistico.\n",
        "\n",
        "Visto l'impossibilità nel lavorare con un insieme composto da tutta la popolazione, è necessario estrarre un campione statistico, operazione chiaramente meno costosa ma porta con sè diverse problematiche. \n",
        "Quando si sceglie un campione bisogna fare in modo che esso rappresenti la popolazione totale per poter trarre conclusioni che poi possono essere probabilisticamente estese.\n",
        "\n",
        "Si introduce quindi il concetto di probabilità: dato un fenomeno aleatorio, con un insieme di risultati tutti egualmente possibili, la probabilità di un evento è definita dal rapporto tra il numero di risultati favorevoli all'evento stesso e il numero di risultati possibili.\n",
        "\n",
        "Il classificatore preso in oggetto si basa sul teorema di Bayes, che afferma che  $P(A|B)=\\frac{P(B|A)P(A)}{P(B)}$  dove:\n",
        "\n",
        "- P(A)  è il prior, il grado iniziale di \"fede\" in A\n",
        "- P(A|B)  è il posteriore, il grado di credenza che rappresenta B\n",
        "- P(B|A)  è la probabilità, il grado di credenza di B, assunto che A sia vero\n",
        "\n",
        "Utilizzando il teorema di Bayes, possiamo quindi trovare la probabilità che A si verifichi, assumendo che B si è verificato. Ergo, in questo caso A è l'ipotesi e B la tesi. L'assunzione che facciamo noi è che i predittori/gli attributi siano indipendenti. Perciò la presenza di una particolare caratteristica non influenza l'altra. Per questo è chiamato \"ingenuo\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl2H-tKChdAr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682
        },
        "outputId": "dd52b4ac-0652-48d8-fd73-9a9ac19e13b8"
      },
      "source": [
        "print(\"Simulazione eseguita con il dataset su cui è stata applicata la PCA e sono stati mantenute 6 componenti:\\n\")\n",
        "score6_nbc = grid_search_cv(nbc, NBC_PARAM, x_train_PCA6, y_train, kf)\n",
        "print_best_scores(score6_nbc, n=5)\n",
        "\n",
        "#print(\"Simulazione eseguita con il dataset su cui è stata applicata la PCA e sono stati mantenute 4 componenti:\\n\")\n",
        "#score4_nbc = grid_search_cv(nbc, NBC_PARAM, x_train_PCA4, y_train, kf)\n",
        "#print_best_scores(score4_nbc, n=5)\n",
        "\n",
        "#print(\"Simulazione eseguita con il dataset su cui non è stata applicata la PCA:\\n\")\n",
        "#score_nbc = grid_search_cv(nbc, NBC_PARAM, X_train_norm, y_train, kf)\n",
        "#print_best_scores(score_nbc, n=5)\n",
        "\n",
        "plot_learning_curve(score6_nbc.best_estimator_, \"Curva di apprendimento di Logistic Regression (6 componenti analizzate)\", x_train_PCA6, y_train, cv=5,name=\"NBC\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Simulazione eseguita con il dataset su cui è stata applicata la PCA e sono stati mantenute 6 componenti:\n",
            "\n",
            "Migliori parametri su insieme di validazione:\n",
            "+------------------+\n",
            "|      Score       |\n",
            "+------------------+\n",
            "| 0.754 (+/-0.028) |\n",
            "+------------------+\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"525px\"\n",
              "            src=\"https://plotly.com/~freguti/442.embed\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fa4d9f4de10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zxyd7lWwIcsF",
        "outputId": "2e391a39-354c-4422-e148-a70ec22e1c71"
      },
      "source": [
        "print_confusion_matrix(score6_nbc, x_test_PCA6, y_test)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+----------------------+------------------+\n",
            "|                 | Predicted not pulsar | Predicted pulsar |\n",
            "+-----------------+----------------------+------------------+\n",
            "| True not pulsar |         2995         |       257        |\n",
            "|   True pulsar   |          56          |       272        |\n",
            "+-----------------+----------------------+------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sUi9OZjhp-E"
      },
      "source": [
        "<a name=\"knn\"></a>\n",
        "###K-Nearest Neighbors Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV7fnxSMIA-O"
      },
      "source": [
        "il KNN è un tipo di apprendimento basato sull'istanza o apprendimento pigro, in cui la funzione viene solo approssimata localmente e tutti i calcoli vengono posticipati fino alla classificazione. L'algoritmo KNN è tra i più semplici di tutti gli algoritmi di apprendimento automatico.\n",
        "\n",
        "La fase di addestramento dell'algoritmo consiste solo nel memorizzare i vettori di caratteristiche e le etichette di classe dei campioni di addestramento. Nella fase di classificazione, K è una costante definita dall'utente e un vettore senza etichetta (una query o un punto di prova) viene classificato assegnando l'etichetta più frequente tra i campioni di addestramento k più vicini a quel punto di ricerca.\n",
        "\n",
        "I parametri del cross validation sono:\n",
        "\n",
        "- n_neighbors: il numero di samples \"vicini\" da analizzare\n",
        "- weights: indica la funzione weight da applicare nella predizione\n",
        " - uniform: tutti i punti \"nel vicinato\" sono pesati in maniera uguale\n",
        " - distance: i punti sono pesati per l'inverso della loro distanza. Perciò, i punti più vicini avranno \n",
        "più peso di quelli più distanti\n",
        "- p: parametro di potenza per la metrica di Minkowski\n",
        " - p=1  si usa la distanza Manhattan ${\\displaystyle L_{1}(P_{1},P_{2})=|x_{1}-x_{2}|+|y_{1}-y_{2}|}$\n",
        " - p=2  si usa la distanza Euclidea ${\\displaystyle L_{2}(P1,P2)={\\sqrt {(x_{1}-x_{2})^{2}+(y_{1}-y_{2})^{2}}}.}$\n",
        " - p>2  è usata la la distanza di Minkowski che tra due punti  ${\\displaystyle P=(x_{1},x_{2},\\ldots ,x_{n})}$ e ${\\displaystyle Q=(y_{1},y_{2},\\ldots ,y_{n})}$ è definita come:\n",
        "  $L_m(P,Q) = \\left(\\sum _{{i=1}}^{n}|x_{i}-y_{i}|^{p}\\right)^{{1/p}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOc4kIs7huxL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "outputId": "ffca1a39-a9a1-4ebf-967a-a027a90a07f5"
      },
      "source": [
        "print(\"Simulazione eseguita con il dataset su cui è stata applicata la PCA e sono stati mantenute 6 componenti:\\n\")\n",
        "score6_knn = grid_search_cv(knn, KNN_PARAM, x_train_PCA6, y_train, kf)\n",
        "print_best_scores(score6_knn, n=5)\n",
        "\n",
        "#print(\"Simulazione eseguita con il dataset su cui è stata applicata la PCA e sono stati mantenute 4 componenti:\\n\")\n",
        "#score4_knn = grid_search_cv(knn, KNN_PARAM, x_train_PCA4, y_train, kf)\n",
        "#print_best_scores(score4_knn, n=5)\n",
        "\n",
        "#print(\"Simulazione eseguita con il dataset su cui non è stata applicata la PCA:\\n\")\n",
        "#score_knn = grid_search_cv(knn, KNN_PARAM, X_train_norm, y_train, kf)\n",
        "#print_best_scores(score_knn, n=5)\n",
        "\n",
        "plot_learning_curve(score6_knn.best_estimator_, \"Curva di apprendimento di Logistic Regression (6 componenti analizzate)\", x_train_PCA6, y_train, cv=5,name=\"KNN\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Simulazione eseguita con il dataset su cui è stata applicata la PCA e sono stati mantenute 6 componenti:\n",
            "\n",
            "Migliori parametri su insieme di validazione:\n",
            "+------------------+------------------+--------+--------------+\n",
            "|      Score       | clf__n_neighbors | clf__p | clf__weights |\n",
            "+------------------+------------------+--------+--------------+\n",
            "| 0.802 (+/-0.031) |        10        |   10   |   uniform    |\n",
            "| 0.799 (+/-0.033) |        2         |   1    |   uniform    |\n",
            "| 0.798 (+/-0.019) |        10        |   1    |   uniform    |\n",
            "| 0.794 (+/-0.036) |        15        |   2    |   distance   |\n",
            "| 0.793 (+/-0.022) |        15        |   2    |   uniform    |\n",
            "+------------------+------------------+--------+--------------+\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"525px\"\n",
              "            src=\"https://plotly.com/~freguti/444.embed\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fa4da338350>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFIuczm6IemC",
        "outputId": "2119c12b-f443-4862-ffc5-ef7d5141b7f4"
      },
      "source": [
        "print_confusion_matrix(score6_knn, x_test_PCA6, y_test)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------+----------------------+------------------+\n",
            "|                 | Predicted not pulsar | Predicted pulsar |\n",
            "+-----------------+----------------------+------------------+\n",
            "| True not pulsar |         3007         |       245        |\n",
            "|   True pulsar   |          48          |       280        |\n",
            "+-----------------+----------------------+------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWqoW1CpjWhC"
      },
      "source": [
        "###ROC Curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVhU-llaQaJM"
      },
      "source": [
        "Arrivati a questo punto dobbiamo comparare le performance dei diversi classificatori. Andiamo quindi a tracciare la ROC curve e la Area Under Curve per tutti i nostri modelli. La ROC curve è tracciata con TPR (True Positive Rate) rispetto a FPR (False Positive Rate) dove TPR è sull'asse y e FPR è sull'asse x. Nello specifico, questi parametri sono:\n",
        "\n",
        "- $TRP/Recall/Sensitivity=\\frac{TP}{TP+FN}$ \n",
        "- $FPR=\\frac{FP}{TN+FP}$ \n",
        "\n",
        "Un modello eccellente ha l'AUC vicino all'1 che significa che ha una buona misura di separabilità. Un modello scadente ha l'AUC vicino allo 0, il che significa che ha la peggiore misura di separabilità."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN_xqP3tjNWF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "outputId": "9a1dad34-1129-4241-8240-c25951402ec8"
      },
      "source": [
        "classifiers = [score6_lr, score6_svm, score6_nbc, score6_knn]\n",
        "classifier_names = [\"Logistic Regression\", \"SVM\", \"GaussianNB\", \"KNN\"]\n",
        "auc_scores, roc_plot = plot_roc_curve(classifiers, classifier_names, \"ROC curve\", x_test_PCA6, y_test)\n",
        "roc_plot"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"550px\"\n",
              "            height=\"550px\"\n",
              "            src=\"https://plotly.com/~freguti/318.embed\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fa4da49a750>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-nKfbCXQY-e"
      },
      "source": [
        "Ora esaminiamo alcuni parametri aggiuntivi che valutano la bontà dei nostri modelli:\n",
        "\n",
        "- Accuratezza\n",
        "- Precisione\n",
        " - $P=\\frac{TP}{TP+FP}$ \n",
        "- Recall\n",
        " - $R=\\frac{TP}{TP+FN}$ \n",
        "- F1 (media ponderata della precisione e recall)\n",
        " - $F1=2∗\\frac{PR}{P+R}$ \n",
        "- Auc, area under the ROC curve\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtR8-lStjcko",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "609ff6ae-7774-437b-8f7a-c952ebd5e392"
      },
      "source": [
        "print_performances(classifiers, classifier_names, auc_scores, x_test_PCA6, y_test)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "      <th>auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Logistic Regression</th>\n",
              "      <td>0.824</td>\n",
              "      <td>0.318</td>\n",
              "      <td>0.802</td>\n",
              "      <td>0.455</td>\n",
              "      <td>0.898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM</th>\n",
              "      <td>0.856</td>\n",
              "      <td>0.325</td>\n",
              "      <td>0.537</td>\n",
              "      <td>0.405</td>\n",
              "      <td>0.882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GaussianNB</th>\n",
              "      <td>0.913</td>\n",
              "      <td>0.514</td>\n",
              "      <td>0.829</td>\n",
              "      <td>0.635</td>\n",
              "      <td>0.935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KNN</th>\n",
              "      <td>0.918</td>\n",
              "      <td>0.533</td>\n",
              "      <td>0.854</td>\n",
              "      <td>0.657</td>\n",
              "      <td>0.958</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    accuracy precision recall     f1    auc\n",
              "Logistic Regression    0.824     0.318  0.802  0.455  0.898\n",
              "SVM                    0.856     0.325  0.537  0.405  0.882\n",
              "GaussianNB             0.913     0.514  0.829  0.635  0.935\n",
              "KNN                    0.918     0.533  0.854  0.657  0.958"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqe1W0b2Rjxt"
      },
      "source": [
        "Possiamo notare come il valore della precision sia molto minore rispetto al valore di recall.\n",
        "\n",
        "Questo è dato dal fatto che il dataset di test sia sbilanciato verso una delel due classi (not pulsar).\n",
        "In valore assoluto avremo più FP rispetto ai TP, abbattendo il valore della precision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fC3VJjQjePZ"
      },
      "source": [
        "<a name=\"conclusioni\"></a>\n",
        "##Conclusioni"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmKVpjiCSjGt"
      },
      "source": [
        "Possiamo notare che il classificatore migliore è il Knn, perché ha l'accuracy più altra e ha inoltre il minor errore sulla predizione delle pulsar.\n",
        "\n",
        "Non è sufficiente analizzare solamente l'accuracy, perché avendo un dataset di test molto sbilanciato potrebbe capitare di avere questo valore molto elevato predicendo semplicemente tutti i campioni come \"not pulsar\", quindi il nostro classificatore deve avere un buon valore di \"precision\".\n",
        "\n",
        "Knn ha dei valori migliori su tutte le categorie, quindi possiamo tranquillamente selezionarlo come migliore."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czrwQ0aeTThF"
      },
      "source": [
        "%%capture\n",
        "!jupyter nbconvert --to html /content/drive/MyDrive/DataSpaces/Progetto_DataSpaces.ipynb"
      ],
      "execution_count": 41,
      "outputs": []
    }
  ]
}